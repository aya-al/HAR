import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn import preprocessing
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.impute import SimpleImputer
import tensorflow as tf
from tensorflow.keras.layers import Conv2D, MaxPool2D
from tensorflow.keras import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization


file=open("C:\\Users\\Aya\\Desktop\\New folder\\WISDM_ar_latest (1)\\WISDM_ar_v1.1\\WISDM_ar_v1.1_raw.txt")
lines=file.readlines()
processedlist=[]
for i,line in enumerate(lines):
    try:
        line = line.split(',')
        last=line[5].split(';')[0]
        last=last.strip()
        if last =='':
            break ;
            
        temp= [line[0],line[1],line[2],line[3],line[4],last]
        processedlist.append(temp)
    except:
        print('error',i)
        


columns=['user', 'activity', 'time', 'x', 'y', 'z']


data=pd.DataFrame(data=processedlist,columns=columns)
data.head()

data.shape

data.info()

data.isnull().sum()

# Handle NaN values by replacing them with the mean of the column
imputer = SimpleImputer(strategy='mean')
data[['x', 'y', 'z']] = imputer.fit_transform(data[['x', 'y', 'z']])


data['activity'].value_counts()

#balancing the data
data['x']=data['x'].astype('float')
data['y']=data['y'].astype('float')
data['z']=data['z'].astype('float')

data.info()

Fs=20

# Handle infinity values by replacing them with NaN and then applying imputation
data.replace([np.inf, -np.inf], np.nan, inplace=True)
data[['x', 'y', 'z']] = imputer.fit_transform(data[['x', 'y', 'z']])

activities =data['activity'].value_counts().index

def plot_activity(activity,data):
    fig,(ax0,ax1,ax2)=plt.subplots(nrows=3,figsize=(15,7),sharex=True)
    plot_axis(ax0,data['time'],data['x'],'X_Axis')
    plot_axis(ax1,data['time'],data['y'],'Y_Axis')
    plot_axis(ax2,data['time'],data['z'],'Z_Axis')
    plt.subplots_adjust(hspace=0.2)
    fig.suptitle(activity)
    plt.subplots_adjust(top=0.90)
    plt.show()
    
def plot_axis(ax,x,y,title):
    ax.plot(x,y,'g')
    ax.set_title(title)
    ax.xaxis.set_visible(False)
    ax.set_ylim([min(y)-np.std(y),max(y)+np.std(y)])
    ax.set_xlim([min(x),max(x)])
    ax.grid(True)
    
for activity in activities:
    data_for_plot=data[(data['activity']==activity)][:Fs*10]
    plot_activity(activity,data_for_plot)

df = data.drop(['user', 'time'], axis = 1).copy()
df.head()

# Use LabelEncoder to encode activity labels
label = LabelEncoder()
df['label'] = label.fit_transform(df['activity'])
df.head()

Walking = df[df['activity']=='Walking'].head(3555).copy()
Jogging = df[df['activity']=='Jogging'].head(3555).copy()
Upstairs = df[df['activity']=='Upstairs'].head(3555).copy()
Downstairs = df[df['activity']=='Downstairs'].head(3555).copy()
Sitting = df[df['activity']=='Sitting'].head(3555).copy()
Standing = df[df['activity']=='Standing'].copy()

balanced_data = pd.DataFrame()
balanced_data = balanced_data.append([Walking, Jogging, Upstairs, Downstairs, Sitting, Standing])
balanced_data.shape

balanced_data['activity'].value_counts()

balanced_data.head()

# Ensure there are no NaN values before one-hot encoding
balanced_data = balanced_data.dropna()

e = OneHotEncoder(sparse=False)
res= e.fit_transform(balanced_data[['label']])
res
# One-hot encode the 'label' column using get_dummies
#one_hot_labels = pd.get_dummies(balanced_data['label'], prefix='label')

# Concatenate the one-hot encoded labels to the DataFrame
#balanced_data = pd.concat([balanced_data, one_hot_labels], axis=1)



balanced_data= pd.get_dummies(balanced_data, columns=['label'])
balanced_data.head()

# Display the updated DataFrame
balanced_data.head()

X = balanced_data[['x', 'y', 'z']]
y = balanced_data[['label_0', 'label_1', 'label_2', 'label_3', 'label_4', 'label_5']]

scaler = StandardScaler()
X = scaler.fit_transform(X)

scaled_X = pd.DataFrame(data=X, columns=['x', 'y', 'z'])
scaled_X = pd.concat([scaled_X, y], axis=1)

Fs = 20
frame_size = Fs * 4  # 80
hop_size = Fs * 2    # 40

def get_frames(df, frame_size, hop_size):
    N_FEATURES = df.shape[1]  # Use the number of features in the original data
    frames = []
    labels = []
    for i in range(0, len(df) - frame_size, hop_size):
        frame = df.values[i: i + frame_size, :]  # Use all features in each frame
        label = df[['label_0', 'label_1', 'label_2', 'label_3', 'label_4', 'label_5']].values[i: i + frame_size].mean(axis=0)
        frames.append(frame)
        labels.append(label)

    frames = np.asarray(frames)
    labels = np.asarray(labels)

    return frames, labels

X, y = get_frames(scaled_X, frame_size, hop_size)


X.shape, y.shape

(3555*6)/40

print("Original shape of X:", scaled_X.shape)
print("Shape of frames:", X.shape)


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

print("Original size of X_train:", X_train.size)
print("Desired size after reshaping:", 621 * 80 * 1)


print("Original shape of X_train:", X_train.shape)
print("Original shape of X_test:", X_test.shape)

# reshape the data based on its size
X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)



print("New shape of X_train:", X_train.shape)
print("New shape of X_test:", X_test.shape)


X_train[0].shape, X_test[0].shape

#reshape the data
#X_train = X_train.reshape(425, 80, 3, 1)
#X_test = X_test.reshape(107, 80, 3, 1)

X_train[0].shape, X_test[0].shape

import tensorflow as tf
from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense


input_shape = X_train[0].shape

# Define the CNN model
model = Sequential()

# Input layer
model.add(Input(shape=input_shape))

# Convolutional Layer 1
model.add(Conv2D(32, kernel_size=(5, 5), activation='relu', input_shape=input_shape,padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))

# Convolutional Layer 2
model.add(Conv2D(256, kernel_size=(5, 5), activation='relu',padding='same'))
model.add(MaxPooling2D(pool_size=(2, 2), strides=(1, 1)))

# Flatten layer to transition from convolutional layers to fully connected layers
model.add(Flatten())

# Fully Connected Layer
model.add(Dense(256, activation='relu'))

# Output Layer
model.add(Dense(6, activation='softmax'))

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Display the model summary
model.summary()




#fitting the data
history = model.fit(X_train, y_train, epochs=93, batch_size=15, validation_data=(X_test, y_test), verbose=1)
#model.compile(optimizer=Adam(learning_rate = 0.001), loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])
#history = model.fit(X_train, y_train, epochs = 93,batch_size=15 ,validation_data= (X_test, y_test), verbose=1)

def plot_learningCurve(history, epochs):
  # Plot training & validation accuracy values
  epoch_range = range(1, epochs+1)
  plt.plot(epoch_range, history.history['accuracy'])
  plt.plot(epoch_range, history.history['val_accuracy'])
  plt.title('Model accuracy')
  plt.ylabel('Accuracy')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'Val'], loc='upper left')
  plt.show()

  # Plot training & validation loss values
  plt.plot(epoch_range, history.history['loss'])
  plt.plot(epoch_range, history.history['val_loss'])
  plt.title('Model loss')
  plt.ylabel('Loss')
  plt.xlabel('Epoch')
  plt.legend(['Train', 'Val'], loc='upper left')
  plt.show()

plot_learningCurve(history,93)


#confusion matric

from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix

"""y_pred = model.predict(X_test)
predicted_classes = np.argmax(y_pred, axis=1)"""

"""mat = confusion_matrix(y_test, predicted_classes)
plot_confusion_matrix(conf_mat=mat, class_names=label.classes_, show_normed=True, figsize=(7,7))"""

model.save_weights('modelweights.h5')
model.save('model.h5')

from tensorflow.keras.models import load_model

model.load_weights('model.h5')

loaded_model = load_model('model.h5')


loaded_model.summary()

"""import numpy as np
from tensorflow.keras.models import load_model

# Define frame_function as an example
def frame_function(data, frame_size):
    frames = []
    for i in range(0, len(data) - frame_size + 1):
        frame = data[i:i+frame_size]
        frames.append(frame)
    return np.array(frames)

# Load the trained model
try:
    model = load_model('model.h5')
except Exception as e:
    print("Error loading the model:", e)

# Function to preprocess live accelerometer data
def preprocess_live_data(live_data):
    # Perform necessary preprocessing steps (scaling, framing, etc.)

    # Example scaling
    scaled_data = live_data / 100.0  # Replace with your scaling factor

    # Example framing (assuming 'frame_size' is a predefined constant)
    framed_data = frame_function(scaled_data, frame_size)

    return framed_data  # Return the preprocessed data

# Function to get real-time predictions
def get_real_time_predictions(live_data):
    # Preprocess live data
    preprocessed_data = preprocess_live_data(live_data)

    # Reshape the data if needed
    preprocessed_data = np.reshape(preprocessed_data, (1, frame_size, 3, 1))

    # Make predictions using the trained model
    predictions = model.predict(preprocessed_data)

    # Get the predicted activity label
    predicted_label = np.argmax(predictions)

    return predicted_label

# Example: Capture live accelerometer data (replace this with your actual data source)
# Generate random accelerometer data for demonstration
live_data =  

# Get real-time predictions
predicted_activity = get_real_time_predictions(live_data)

# Output the predicted activity (replace this with your desired output mechanism)
print("Predicted Activity:", predicted_activity)
"""

"""import numpy as np
from tensorflow.keras.models import load_model

# Load the trained model
try:
    model = load_model('model.h5')
except Exception as e:
    print("Error loading the model:", e)

# Function to preprocess live accelerometer data
def preprocess_live_data(live_data):

    # Example scaling
    scaled_data = live_data / 100.0 

    # Example framing (assuming 'frame_size' is a predefined constant)
    framed_data = frame_function(scaled_data, frame_size)

    return framed_data  # Return the preprocessed data

# Function to get real-time predictions
def get_real_time_predictions(live_data):
    # Preprocess live data
    preprocessed_data = preprocess_live_data(live_data)

    # Reshape the data if needed
    preprocessed_data = np.reshape(preprocessed_data, (1, frame_size, 3, 1))

    # Make predictions using the trained model
    predictions = model.predict(preprocessed_data)

    # Get the predicted activity label
    predicted_label = np.argmax(predictions)

    # Check if the predicted activity corresponds to an active state
    active_states = ['Walking', 'Jogging', 'Upstairs', 'Downstairs']
    is_active = label.classes_[predicted_label] in active_states

    return is_active


live_data =   np.random.rand(frame_size, 3)

# Get real-time predictions
is_active =  get_real_time_predictions(live_data)

# Output whether the person is active based on the predicted activity
print("Is the person active?", is_active)
"""

# ... (previous code)

# Check the shapes before reshaping
print("Original shape of X_train:", X_train.shape)
print("Original shape of y_train:", y_train.shape)

# Reshape X_train
X_train_reshaped = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)

# Reshape y_train
y_train_reshaped = y_train.reshape(y_train.shape[0], 1, 1, y_train.shape[1])

# Check the shapes after reshaping
print("New shape of X_train:", X_train_reshaped.shape)
print("New shape of y_train:", y_train_reshaped.shape)

# ... (continue with the rest of your code)


# ... (previous code)

# Ensure that y_train has the correct shape
y_train_reshaped = y_train.reshape((y_train.shape[0], 1, 1, y_train.shape[1]))

# Stochastic Gradient Descent (SGD) algorithm
def stochastic_gradient_descent(X_train, y_train, learning_rate, epochs):
    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)

    for epoch in range(epochs):
        for i in range(len(X_train)):
            # Select a random sample
            random_index = np.random.randint(0, len(X_train))
            X_sample, y_sample = X_train[random_index], y_train[random_index]

            # Reshape X_sample to match the model input shape
            X_sample_reshaped = np.reshape(X_sample, (1, frame_size, 9, 1))  # Adjust the shape based on your input features

            # Forward pass
            with tf.GradientTape() as tape:
                predictions = model(X_sample_reshaped)
                loss = tf.keras.losses.categorical_crossentropy(np.reshape(y_sample, (1, 6)), predictions)

            # Compute gradients
            gradients = tape.gradient(loss, model.trainable_variables)

            # Update parameters using the gradients and learning rate
            optimizer.apply_gradients(zip(gradients, model.trainable_variables))

# Set SGD hyperparameters
learning_rate = 0.001
epochs_sgd = 93

# Apply the SGD algorithm
stochastic_gradient_descent(X_train, y_train_reshaped, learning_rate, epochs_sgd)

# ... (remaining code)




























